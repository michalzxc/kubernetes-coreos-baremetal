#cloud-config

hostname: worker4
# include one or more SSH public keys
ssh_authorized_keys:
  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDZYL/631qBTScS+sQb9L7GKHczu1c9+ObEsmE5xyQBF9g1i0SiB/v1IxligpGMSFu1squfjsOZrBdGGCBaZ9rJQt7q1Uvi95QXA7ka6sAbxPvAppWypkutdytX9PY009C25SXzhG8Wo9MbcZF8g0dYJEyPnC+7OMf0Uz5EsqFGxSWhIqUSAc92mMye2YkI0Y1y+HUJUNAYjOK7rnaV34yL+bqOc3HQ/lFoCEpJ7AQmj+373z/CLRGJo+6ckBkptlFU2jCCbjeXE998MtBQ/OhjdibJBiQWuwiCf8ZQ1ggAVz4vl5gvMYRA9fC9MFW1ke2BbpnkAJZeUBDwM//TsK8R MonkeySphere2016-12-06T16:05:42 Michal Rydlikowski (Personal Secondary Email Address) <Michal.Rydlikowski@gmail.com>
  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCqtzL0th+fvk8YFDr+r4evH8s/xEAtXqEEPJN0ldyESyO/H64az4QRYaZTgwbLR6nY9Zk0VBH1UQ2M35flYH1ZMQViDaZ/CKXoctLmCgWSc05IYQUf/afruBKMmCvEJ1YAqWyY1h9rtNX2amICygxzEdFG/Pg1tiM4reTgTFxQOu15knCWKoYzZQ6WhUcvFU+E7d/OFYEzIAil7oAZiDzVPRnNY/HLPqnlisBy1HNiBqaX/nM3tDyxYPLDvROS0O6jhUzXJ1VK72cJskwHusLWsbLRTtKdRX9WX+4yYCzJEoRKYTKRLBYInkHqLjOYs9vM/SovgGI9wtvRgeDTfFyB michal.rydlikowski@concreteplatform.com
  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDgx5rf9UoTbmMPZa/ZeJnbQj+4VlIkDtoMT9YP1Fda//Jm9Pc2iC/v7azmaJZvkGw8wqIct0RUO0fQgxV/KpUtmKwwgtkhlcBkrOPxYqsVWk/avCPyx0RVlmNbclIB6jQ+q5Z90J5m6ygGlvrUKUOFfZMVluqWgH32oZzvEiKd4VbjA8/6C7lC8xhNcpNSU1I3K7OnpX14+HJSuk0d/CO3Gzgy/g0A871UqWkC+olGswD6AqSsUhKZobcDUe1LbL4u5WKSZbowzsvHiBw+aXhD04pUQ7U0wylk09IzrXsPV0TF+PeS/mi0wOj6x7JD6xW9UpGL1x9uFEVwxCr92f9i06j0pDWmP1Ju89tp12NMxAgkTkJknCioZNEtrWluGQMkHMvOyxVS/hdJUvNFMd1RK3eJftJG5+1dAYcfhLBuy6AXwmsJpgoK6YNDuFO8/wqh7mJff9JXLZqt/QHKOZGbnRLzRW0dQPa5DVUeCvIanPQ/gs++8WZPcZaCDqJbOh7cJ8fYU7Q6iiPWlLjb82QjB5z/DW5qN5Tba0nRNEFTFuTXSXlw7S66vjqfscaT+dNJ3rCai3Wn1i94tdywBgAhsT3Dm2BrLKbJ17QOH9cR0CxY5acicSu7+FELUjUsttHC7OLZ3sWxhmu0aZHDrktsc4qgjiuBNbRj2SN6WjedJQ== henry.dobson@concreteplatform.com
  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCkFdtiCsOT+gu7T9paGzP9Uoe5XiirlEnXR7aUTQd/l+QQ1stB8pbH7nlTH2tTw2hsjLAOnyXnDzJP3YiI11ZJPzFzzOKK8yyhi32eT1Be576dqOdMnw5j+btdp3T10Nmot3qM6p512f14nw0B2xdK2OvSm2o6/RoH9zbYVMy1klXm7way+JlmWsYODRVftuskSUr0pTCY1LkKBQr/23yuCtP32gGQiPSQ6Cvp2Q7mLtdmV5jgcfFmH9kQfx88mZdxSXzjTivyoD6dt6u9rtaX3AuGnv2ivV2FV5PPWGDUxnDIUzZumt1pdriI1SUVECJo+UBAKT46hyUx7dPMufRR justin.miller@concreteplatform.com
users:
  - name: "michalzxc"
    passwd: "$6$BP4OCopq$QZujrmUwUQvD3GAGrjNOLJLz.o0./MXxL3KM1509jgxd2RxRnia75G8QL2cf72B5rTxycg30zGIx61lNyoqtN0"
    groups:
     - "sudo"
     - "docker"
    ssh_authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDZYL/631qBTScS+sQb9L7GKHczu1c9+ObEsmE5xyQBF9g1i0SiB/v1IxligpGMSFu1squfjsOZrBdGGCBaZ9rJQt7q1Uvi95QXA7ka6sAbxPvAppWypkutdytX9PY009C25SXzhG8Wo9MbcZF8g0dYJEyPnC+7OMf0Uz5EsqFGxSWhIqUSAc92mMye2YkI0Y1y+HUJUNAYjOK7rnaV34yL+bqOc3HQ/lFoCEpJ7AQmj+373z/CLRGJo+6ckBkptlFU2jCCbjeXE998MtBQ/OhjdibJBiQWuwiCf8ZQ1ggAVz4vl5gvMYRA9fC9MFW1ke2BbpnkAJZeUBDwM//TsK8R MonkeySphere2016-12-06T16:05:42 Michal Rydlikowski (Personal Secondary Email Address) <Michal.Rydlikowski@gmail.com>
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCqtzL0th+fvk8YFDr+r4evH8s/xEAtXqEEPJN0ldyESyO/H64az4QRYaZTgwbLR6nY9Zk0VBH1UQ2M35flYH1ZMQViDaZ/CKXoctLmCgWSc05IYQUf/afruBKMmCvEJ1YAqWyY1h9rtNX2amICygxzEdFG/Pg1tiM4reTgTFxQOu15knCWKoYzZQ6WhUcvFU+E7d/OFYEzIAil7oAZiDzVPRnNY/HLPqnlisBy1HNiBqaX/nM3tDyxYPLDvROS0O6jhUzXJ1VK72cJskwHusLWsbLRTtKdRX9WX+4yYCzJEoRKYTKRLBYInkHqLjOYs9vM/SovgGI9wtvRgeDTfFyB michal.rydlikowski@concreteplatform.com
write_files:
  - path: /etc/kubernetes/install.sh
    owner: "root"
    permissions: 0700
    content: |
      #!/bin/bash
      set -e
      
      # List of etcd servers (http://ip:port), comma separated
      export ETCD_ENDPOINTS=http://192.168.115.2:2379,http://192.168.115.3:2379,http://192.168.115.4:2379
      
      # The endpoint the worker node should use to contact controller nodes (https://ip:port)
      # In HA configurations this should be an external DNS record or loadbalancer in front of the control nodes.
      # However, it is also possible to point directly to a single control node.
      export CONTROLLER_ENDPOINT=http://127.0.0.1:8182
      
      # Specify the version (vX.Y.Z) of Kubernetes assets to deploy
      export K8S_VER=v1.7.8_coreos.2
      
      # Hyperkube image repository to use.
      export HYPERKUBE_IMAGE_REPO=quay.io/coreos/hyperkube
      
      # The CIDR network to use for pod IPs.
      # Each pod launched in the cluster will be assigned an IP out of this range.
      # Each node will be configured such that these IPs will be routable using the flannel overlay network.
      export POD_NETWORK=10.2.0.0/16
      
      # The IP address of the cluster DNS service.
      # This must be the same DNS_SERVICE_IP used when configuring the controller nodes.
      export DNS_SERVICE_IP=10.3.0.10
      
      # Whether to use Calico for Kubernetes network policy.
      export USE_CALICO=true
      
      # Determines the container runtime for kubernetes to use. Accepts 'docker' or 'rkt'.
      export CONTAINER_RUNTIME=docker
      
      # The above settings can optionally be overridden using an environment file:
      ENV_FILE=/run/coreos-kubernetes/options.env
      
      # To run a self hosted Calico install it needs to be able to write to the CNI dir
      if [ "${USE_CALICO}" = "true" ]; then
          export CALICO_OPTS="--volume cni-bin,kind=host,source=/opt/cni/bin \
                              --mount volume=cni-bin,target=/opt/cni/bin"
      else
          export CALICO_OPTS=""
      fi
      
      # -------------
      
      function init_config {
          local REQUIRED=( 'ADVERTISE_IP' 'ETCD_ENDPOINTS' 'CONTROLLER_ENDPOINT' 'DNS_SERVICE_IP' 'K8S_VER' 'HYPERKUBE_IMAGE_REPO' 'USE_CALICO' )
      
          if [ -f $ENV_FILE ]; then
              export $(cat $ENV_FILE | xargs)
          fi
      
          if [ -z $ADVERTISE_IP ]; then
              export ADVERTISE_IP=$(awk -F= '/COREOS_PUBLIC_IPV4/ {print $2}' /etc/environment)
          fi
      
          for REQ in "${REQUIRED[@]}"; do
              if [ -z "$(eval echo \$$REQ)" ]; then
                  echo "Missing required config value: ${REQ}"
                  exit 1
              fi
          done
      }
      
      function init_templates {
          local TEMPLATE=/etc/systemd/system/kubelet.service
          local uuid_file="/var/run/kubelet-pod.uuid"
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      [Service]
      Environment=KUBELET_IMAGE_TAG=${K8S_VER}
      Environment=KUBELET_IMAGE_URL=${HYPERKUBE_IMAGE_REPO}
      Environment="RKT_RUN_ARGS=--uuid-file-save=${uuid_file} \
        --volume dns,kind=host,source=/etc/resolv.conf \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume rkt,kind=host,source=/opt/bin/host-rkt \
        --mount volume=rkt,target=/usr/bin/rkt \
        --volume var-lib-rkt,kind=host,source=/var/lib/rkt \
        --mount volume=var-lib-rkt,target=/var/lib/rkt \
        --volume stage,kind=host,source=/tmp \
        --mount volume=stage,target=/tmp \
        --volume var-log,kind=host,source=/var/log \
        --mount volume=var-log,target=/var/log \
        ${CALICO_OPTS}"
      ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
      ExecStartPre=/usr/bin/mkdir -p /var/log/containers
      ExecStartPre=-/usr/bin/rkt rm --uuid-file=${uuid_file}
      ExecStartPre=/usr/bin/mkdir -p /opt/cni/bin
      ExecStart=/usr/lib/coreos/kubelet-wrapper \
        --container-runtime=${CONTAINER_RUNTIME} \
        --hostname-override=%HOST% \
        --require-kubeconfig=true \
        --rkt-path=/usr/bin/rkt \
        --rkt-stage1-image=coreos.com/rkt/stage1-coreos \
        --register-node=true \
        --allow-privileged=true \
        --pod-manifest-path=/etc/kubernetes/manifests \
        --hostname-override=${ADVERTISE_IP} \
        --cluster_dns=${DNS_SERVICE_IP} \
        --cluster_domain=cluster.local \
        --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml \
        --tls-cert-file=/etc/kubernetes/ssl/worker.pem \
        --tls-private-key-file=/etc/kubernetes/ssl/worker-key.pem
      ExecStop=-/usr/bin/rkt stop --uuid-file=${uuid_file}
      Restart=always
      RestartSec=10
      
      [Install]
      WantedBy=multi-user.target
      EOF
          fi
      
          local TEMPLATE=/opt/bin/host-rkt
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      #!/bin/sh
      # This is bind mounted into the kubelet rootfs and all rkt shell-outs go
      # through this rkt wrapper. It essentially enters the host mount namespace
      # (which it is already in) only for the purpose of breaking out of the chroot
      # before calling rkt. It makes things like rkt gc work and avoids bind mounting
      # in certain rkt filesystem dependancies into the kubelet rootfs. This can
      # eventually be obviated when the write-api stuff gets upstream and rkt gc is
      # through the api-server. Related issue:
      # https://github.com/coreos/rkt/issues/2878
      exec nsenter -m -u -i -n -p -t 1 -- /usr/bin/rkt "\$@"
      EOF
          fi
      
          local TEMPLATE=/etc/systemd/system/load-rkt-stage1.service
          if [ ${CONTAINER_RUNTIME} = "rkt" ] && [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      [Unit]
      Description=Load rkt stage1 images
      Documentation=http://github.com/coreos/rkt
      Requires=network-online.target
      After=network-online.target
      Before=rkt-api.service
      
      [Service]
      Type=oneshot
      RemainAfterExit=yes
      ExecStart=/usr/bin/rkt fetch /usr/lib/rkt/stage1-images/stage1-coreos.aci /usr/lib/rkt/stage1-images/stage1-fly.aci  --insecure-options=image
      
      [Install]
      RequiredBy=rkt-api.service
      EOF
          fi
      
          local TEMPLATE=/etc/systemd/system/rkt-api.service
          if [ ${CONTAINER_RUNTIME} = "rkt" ] && [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      [Unit]
      Before=kubelet.service
      
      [Service]
      ExecStart=/usr/bin/rkt api-service
      Restart=always
      RestartSec=10
      
      [Install]
      RequiredBy=kubelet.service
      EOF
          fi
      
          local TEMPLATE=/etc/kubernetes/worker-kubeconfig.yaml
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          certificate-authority: /etc/kubernetes/ssl/ca.pem
          server: ${CONTROLLER_ENDPOINT}
      users:
      - name: kubelet
        user:
          client-certificate: /etc/kubernetes/ssl/worker.pem
          client-key: /etc/kubernetes/ssl/worker-key.pem
      contexts:
      - context:
          cluster: local
          user: kubelet
        name: kubelet-context
      current-context: kubelet-context
      EOF
          fi
      
          local TEMPLATE=/etc/kubernetes/manifests/kube-proxy.yaml
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-proxy
        namespace: kube-system
        annotations:
          rkt.alpha.kubernetes.io/stage1-name-override: coreos.com/rkt/stage1-fly
      spec:
        hostNetwork: true
        containers:
        - name: kube-proxy
          image: ${HYPERKUBE_IMAGE_REPO}:$K8S_VER
          command:
          - /hyperkube
          - proxy
          - --master=${CONTROLLER_ENDPOINT}
          - --cluster-cidr=${POD_NETWORK}
          - --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml
          securityContext:
            privileged: true
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: "ssl-certs"
          - mountPath: /etc/kubernetes/worker-kubeconfig.yaml
            name: "kubeconfig"
            readOnly: true
          - mountPath: /etc/kubernetes/ssl
            name: "etc-kube-ssl"
            readOnly: true
          - mountPath: /var/run/dbus
            name: dbus
            readOnly: false
        volumes:
        - name: "ssl-certs"
          hostPath:
            path: "/usr/share/ca-certificates"
        - name: "kubeconfig"
          hostPath:
            path: "/etc/kubernetes/worker-kubeconfig.yaml"
        - name: "etc-kube-ssl"
          hostPath:
            path: "/etc/kubernetes/ssl"
        - hostPath:
            path: /var/run/dbus
          name: dbus
      EOF
          fi
      
          local TEMPLATE=/etc/flannel/options.env
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      FLANNELD_IFACE=$ADVERTISE_IP
      FLANNELD_ETCD_ENDPOINTS=$ETCD_ENDPOINTS
      EOF
          fi
      
          local TEMPLATE=/etc/systemd/system/flanneld.service.d/40-ExecStartPre-symlink.conf.conf
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      [Service]
      ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
      EOF
          fi
      
          local TEMPLATE=/etc/systemd/system/docker.service.d/40-flannel.conf
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      [Unit]
      Requires=flanneld.service
      After=flanneld.service
      [Service]
      EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
      EOF
          fi
      
          local TEMPLATE=/etc/kubernetes/cni/docker_opts_cni.env
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      DOCKER_OPT_BIP=""
      DOCKER_OPT_IPMASQ=""
      EOF
      
          fi
      
          local TEMPLATE=/etc/kubernetes/cni/net.d/10-flannel.conf
          if [ "${USE_CALICO}" = "false" ] && [ ! -f "${TEMPLATE}" ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      {
          "name": "podnet",
          "type": "flannel",
          "delegate": {
              "isDefaultGateway": true
          }
      }
      EOF
          fi
      
      }
      
      systemctl enable haproxy.service
      systemctl start haproxy.service
      
      init_config
      init_templates
      
      chmod +x /opt/bin/host-rkt
      
      systemctl stop update-engine; systemctl mask update-engine
      
      systemctl daemon-reload
      
      if [ $CONTAINER_RUNTIME = "rkt" ]; then
              systemctl enable load-rkt-stage1
              systemctl enable rkt-api
      fi
      
      systemctl enable flanneld; systemctl start flanneld
      
      
      systemctl enable kubelet; systemctl start kubelet

  - path: /etc/kubernetes/ssl/ca.pem
    owner: "root"
    permissions: 0600
    content: |
      -----BEGIN CERTIFICATE-----
      MIIC+jCCAeKgAwIBAgIJAPh17scJCGbBMA0GCSqGSIb3DQEBCwUAMBIxEDAOBgNV
      BAMMB2t1YmUtY2EwHhcNMTcxMDI4MTM1NzM3WhcNNDUwMzE1MTM1NzM3WjASMRAw
      DgYDVQQDDAdrdWJlLWNhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA
      xRbtt+vFNE/FsiQh1T/Tj9nM1Oj+De1AeilFeFLWqu0OTQAHS2d9hzJJ11Nbb+ve
      Susj52LRJKK6Z7sFvXtGCiQ8rVJrKSgvQrejsXAI37L/QHPDlxVj3tEXFcQITlai
      RGryZBNkQnyQtBUVY7klPfLMEMlWITLUUd40KGGThSO40O7pb1k2qa0cGxhfC6Om
      8ykxs+mZrYKrb+BhH/8iGVubKvbMAiqSk+GWtsEq3Z0VeJo1lk7iW9PtV5VtZez8
      6a+1cv0Z+DUysfDkPTAFCq0bGWz9ezMLvqB+3O5I24jfc+1HnSxXehvC0FvEuTpe
      GBOOrj6h1ZLPWbbUGlcmBwIDAQABo1MwUTAdBgNVHQ4EFgQUyBK9vD42B5puimsJ
      0pfqVnPDRk0wHwYDVR0jBBgwFoAUyBK9vD42B5puimsJ0pfqVnPDRk0wDwYDVR0T
      AQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAOF9AT0Ejzlowl154tCqBgBzZ
      zqwihPmiKlScIEPpwmcp+xG/xfAhnekMxK5uONLZrdEjbofKkXGIGtY0DpEwdNK8
      WXMQQlz37pOB/NwDmRW/HN4kKzLqnRWwUHiwL8FrtDJ1Z9WerzRArb9ARd7IMYwO
      xo3p6NKvO849aFdmlY+Agm2AZxHJGjTQVJR2CdtF9XDu9z1EkwB3mKILf9eqeN0w
      2JywAlOZQhz0qiSQPDEgBv7GeCvqnyJ3gdE41ZGgAiAlODSMtqYVeaSjHIZw+LNA
      Mm/vt2HvYiMnGNjmTfOmentc3tLAeTIEzxvINmzVhSuv3AAsrUxdgttrWBPfhQ==
      -----END CERTIFICATE-----

  - path: /etc/kubernetes/ssl/worker.pem
    owner: "root"
    permissions: 0600
    content: |
      -----BEGIN CERTIFICATE-----
      MIIC0jCCAbqgAwIBAgIJAL2GgDxUDuYzMA0GCSqGSIb3DQEBCwUAMBIxEDAOBgNV
      BAMMB2t1YmUtY2EwHhcNMTcxMDI4MTM1NzU1WhcNMjcxMDI2MTM1NzU1WjASMRAw
      DgYDVQQDDAd3b3JrZXI0MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA
      xHv4/nxYImAvFQw+1kqvFTwkXjySNcaeD1BDz2/oPskRbkbx4JYWoeYNjbLXHRpG
      CbWYYEGA61gtQXm6y8D8denraHbpRhy7O1FI6/btTY0bRYG68jHXbSDDN49Qjc6N
      M+g7T05ONvRP0WAiLXAp6xtFnqHX7dY6iweqUYR32Gd68y1AT2D+vyNhQuxcVeZS
      EH2qani2UyK4T0xwOFUu3eHhndGxaTF9zq0jlbEkALT3zwXLrphs9RmSXHHyKlVy
      /jnwkjKAo7yxMOxo5REsg8q4liCcb8hTGI9wE5zPtu0x2ePSXacWlPRshW4aFM/K
      KUXQSUfPdZJIxllsr5WWIwIDAQABoyswKTAJBgNVHRMEAjAAMAsGA1UdDwQEAwIF
      4DAPBgNVHREECDAGhwTAqHMFMA0GCSqGSIb3DQEBCwUAA4IBAQAKpxT5vg8fO/Tf
      f+YJHCrI98URefjZuN41xG6CdKp6BJfNaJetqhiHm0E9inI4ryhTdLUqdGeTegea
      fwjSVv2ckM6GZWqql9dYGLCR+iW7N5T9fb3r610wXrE0qAWvhoO0Rt+KmB8256O8
      QWTUt+Hl14oN6QXFyeMUsGnYiVBOq1XF3jTQxGjIMXbxF394UfrCj/VPp/NE4oRP
      94HPMVEaqfD4g9jc6DhtgkSsR63s4QObllQFsT5GMWPaR4IJYbpus/O64nbRnXMs
      GAET2yXuhjGmOKfCPdfifcHEmqAVLyj6wcCli0WzzMY/uRagC0p27ft/FidbuMa8
      rYxJsNYT
      -----END CERTIFICATE-----

  - path: /etc/kubernetes/ssl/worker-key.pem
    owner: "root"
    permissions: 0600
    content: |
      -----BEGIN RSA PRIVATE KEY-----
      MIIEpAIBAAKCAQEAxHv4/nxYImAvFQw+1kqvFTwkXjySNcaeD1BDz2/oPskRbkbx
      4JYWoeYNjbLXHRpGCbWYYEGA61gtQXm6y8D8denraHbpRhy7O1FI6/btTY0bRYG6
      8jHXbSDDN49Qjc6NM+g7T05ONvRP0WAiLXAp6xtFnqHX7dY6iweqUYR32Gd68y1A
      T2D+vyNhQuxcVeZSEH2qani2UyK4T0xwOFUu3eHhndGxaTF9zq0jlbEkALT3zwXL
      rphs9RmSXHHyKlVy/jnwkjKAo7yxMOxo5REsg8q4liCcb8hTGI9wE5zPtu0x2ePS
      XacWlPRshW4aFM/KKUXQSUfPdZJIxllsr5WWIwIDAQABAoIBAFa76yRbu4QhNWdz
      KmHqQfN7wwoebuRyMt/LrMNrQPhGRf2KL8fWOVNqfF0qry7XpEcw9Hy5LbG2EPQw
      Sjcbj9Z1VkedBki+8h+zOB+BtyteuMUsCVxUiCZSKhrilsqifU6Gm3fct0Ym/v0M
      FMo+EyMHvOnTw7/nq5wn3BV0BntuHE/qJ3CH6aCB7AmZzcTVnXCk34nQX38Y0fVQ
      aYdyyhRV6Wq8ZnnwmmTQKj61zB0+8LwQw72z0CVZELBjL5NIvDXkrkckBhQs8M2F
      OF/HsK+m8FI44mYAztrUI7kIUZLPTX9b9ZwJn3ekXuhLWOLBm12jvsaoTHYbuNpp
      r5cYzPECgYEA7ZZFJnsVwRmLJ9L8F/Khunc0ffLpXIN1J3FH9gqxrNa23TFMm6Pf
      qEp/AApU0lvjudOXST9BXuhMGwHzVXYqgRwBBAM56tJj9IrTCCZN3fADckMV1BiX
      EeUvLXzUd6ZuAFOWZUETofHycfzUvlBHrap9Oy/k1u9N6ZjTb3w94DcCgYEA07Y5
      SW433Ddbw3ejdsAUxSIWd7Ze3V1GqTfm3Hk9xFUA4XuHf7mcn1Se/SroH/3zyeAF
      prYitLNmW6S4fAwI+Cku2jmchxdix9j2cXX4jkcdTeyYlx1qclnXqbRI3FigLfqr
      v1YHt6pIcmC5NEhk8cwB3eecUdweMF2fGjNSS3UCgYEAn70b6x5wZu8kp3bsdbF1
      WWhSS6gpjED6YLWCW4OfKAUI17kQt8g7VOwZbU6E7xZHCyafHelAq23l1Xa/QyN6
      umm4qRj+NMqV19IKxvzx95kqyAUgxQRvdeGwkZXPSE8GwINyp8tURtrkVg1WgEp7
      luD85/Fwm64zpS5SYi+XbosCgYEAmYxUSDm0MffEiiQ/XmU5qzBp8zFF5G8+TITK
      36Tfc4HjPaaGQ9CvE7AgD6YK7QkWw5fDAYKf8UccOeNiGjXwW2wjGwslx6Tic+eb
      9faF60sQqe+rTA59oaofjGnBXm643iAsaH1Q45iRKsOUD6Nh8yzvPa455fUrwBBm
      0G3ghfECgYAUm0zyI859m5wvjAKfCNflh+b+FI8lS85bZJfE8lnuASb3QmWDE2L+
      PIQE4uCvCDvCAFBfmEpAQDeT7UcejeNJk16kDp7ZupKHZMLsF+9aPyoSLGqqkR8S
      f7aqnE9u5ctjrxtOyIIVchcHmJYEDdheHc0NbNrzrJsfUDilkINw+w==
      -----END RSA PRIVATE KEY-----

  - path: /etc/kubernetes/cni/docker_opts_cni.env
coreos:
  units:
    - name: update-engine.service
      mask: true
      command: stop
    - name: locksmithd.service
      mask: true
      command: stop
    - name: 00-worker4.network
      content: |
        [Match]
        Name=eth0

        [Network]
        Address=192.168.115.5/24
        Gateway=192.168.115.1
        DNS=8.8.8.8
    - name: fleet.service
      command: start
    - name: etcd2.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=etcd2
        Conflicts=etcd.service
        [Service]
        User=etcd
        Type=notify
        Environment="ETCD_ADVERTISE_CLIENT_URLS=http://192.168.115.2:2379"
        Environment="ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379"
        Environment=ETCD_DATA_DIR=/var/lib/etcd2
        Environment=ETCD_NAME=worker4
        ExecStart=/usr/bin/etcd2
        Restart=always
        RestartSec=10s
        LimitNOFILE=40000
        TimeoutStartSec=0
        [Install]
        WantedBy=multi-user.target
    - name: haproxy.service
      enable: false
      content: |
        [Unit]
        Description=HAPROXY Kubernetes API
        After=docker.service
        Requires=docker.service

        [Service]
        ExecStart=/usr/bin/docker run --rm -e "MASTERS=192.168.115.2:443 192.168.115.3:443 192.168.115.4:443" -p 127.0.0.1:8182:8182 --name kubeapihaproxy --cap-add=SYS_ADMIN --cap-add DAC_READ_SEARCH --tmpfs /run --tmpfs /run/lock -v /sys/fs/cgroup:/sys/fs/cgroup:ro -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/:ro concreteplatform/kubeapihaproxy
        ExecStop=/usr/bin/docker stop kubeapihaproxy

        [Install]
        WantedBy=multi-user.target
    - name: fleet.service
      command: start
      enable: true
    - name: kubeinstall.service
      command: start
      content: |
        [Unit]
        Description=K8S installer
        After=etcd2.service
        Requires=etcd2.service

        [Service]
        Type=oneshot
        Environment=ADVERTISE_IP=192.168.115.5
        ExecStart=/etc/kubernetes/install.sh
