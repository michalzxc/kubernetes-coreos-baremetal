#cloud-config

hostname: controller2
# include one or more SSH public keys
ssh_authorized_keys:
  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDZYL/631qBTScS+sQb9L7GKHczu1c9+ObEsmE5xyQBF9g1i0SiB/v1IxligpGMSFu1squfjsOZrBdGGCBaZ9rJQt7q1Uvi95QXA7ka6sAbxPvAppWypkutdytX9PY009C25SXzhG8Wo9MbcZF8g0dYJEyPnC+7OMf0Uz5EsqFGxSWhIqUSAc92mMye2YkI0Y1y+HUJUNAYjOK7rnaV34yL+bqOc3HQ/lFoCEpJ7AQmj+373z/CLRGJo+6ckBkptlFU2jCCbjeXE998MtBQ/OhjdibJBiQWuwiCf8ZQ1ggAVz4vl5gvMYRA9fC9MFW1ke2BbpnkAJZeUBDwM//TsK8R MonkeySphere2016-12-06T16:05:42 Michal Rydlikowski (Personal Secondary Email Address) <Michal.Rydlikowski@gmail.com>
  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCqtzL0th+fvk8YFDr+r4evH8s/xEAtXqEEPJN0ldyESyO/H64az4QRYaZTgwbLR6nY9Zk0VBH1UQ2M35flYH1ZMQViDaZ/CKXoctLmCgWSc05IYQUf/afruBKMmCvEJ1YAqWyY1h9rtNX2amICygxzEdFG/Pg1tiM4reTgTFxQOu15knCWKoYzZQ6WhUcvFU+E7d/OFYEzIAil7oAZiDzVPRnNY/HLPqnlisBy1HNiBqaX/nM3tDyxYPLDvROS0O6jhUzXJ1VK72cJskwHusLWsbLRTtKdRX9WX+4yYCzJEoRKYTKRLBYInkHqLjOYs9vM/SovgGI9wtvRgeDTfFyB michal.rydlikowski@concreteplatform.com
  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDgx5rf9UoTbmMPZa/ZeJnbQj+4VlIkDtoMT9YP1Fda//Jm9Pc2iC/v7azmaJZvkGw8wqIct0RUO0fQgxV/KpUtmKwwgtkhlcBkrOPxYqsVWk/avCPyx0RVlmNbclIB6jQ+q5Z90J5m6ygGlvrUKUOFfZMVluqWgH32oZzvEiKd4VbjA8/6C7lC8xhNcpNSU1I3K7OnpX14+HJSuk0d/CO3Gzgy/g0A871UqWkC+olGswD6AqSsUhKZobcDUe1LbL4u5WKSZbowzsvHiBw+aXhD04pUQ7U0wylk09IzrXsPV0TF+PeS/mi0wOj6x7JD6xW9UpGL1x9uFEVwxCr92f9i06j0pDWmP1Ju89tp12NMxAgkTkJknCioZNEtrWluGQMkHMvOyxVS/hdJUvNFMd1RK3eJftJG5+1dAYcfhLBuy6AXwmsJpgoK6YNDuFO8/wqh7mJff9JXLZqt/QHKOZGbnRLzRW0dQPa5DVUeCvIanPQ/gs++8WZPcZaCDqJbOh7cJ8fYU7Q6iiPWlLjb82QjB5z/DW5qN5Tba0nRNEFTFuTXSXlw7S66vjqfscaT+dNJ3rCai3Wn1i94tdywBgAhsT3Dm2BrLKbJ17QOH9cR0CxY5acicSu7+FELUjUsttHC7OLZ3sWxhmu0aZHDrktsc4qgjiuBNbRj2SN6WjedJQ== henry.dobson@concreteplatform.com
  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCkFdtiCsOT+gu7T9paGzP9Uoe5XiirlEnXR7aUTQd/l+QQ1stB8pbH7nlTH2tTw2hsjLAOnyXnDzJP3YiI11ZJPzFzzOKK8yyhi32eT1Be576dqOdMnw5j+btdp3T10Nmot3qM6p512f14nw0B2xdK2OvSm2o6/RoH9zbYVMy1klXm7way+JlmWsYODRVftuskSUr0pTCY1LkKBQr/23yuCtP32gGQiPSQ6Cvp2Q7mLtdmV5jgcfFmH9kQfx88mZdxSXzjTivyoD6dt6u9rtaX3AuGnv2ivV2FV5PPWGDUxnDIUzZumt1pdriI1SUVECJo+UBAKT46hyUx7dPMufRR justin.miller@concreteplatform.com
users:
  - name: "michalzxc"
    passwd: "$6$BP4OCopq$QZujrmUwUQvD3GAGrjNOLJLz.o0./MXxL3KM1509jgxd2RxRnia75G8QL2cf72B5rTxycg30zGIx61lNyoqtN0"
    groups:
     - "sudo"
     - "docker"
    ssh_authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDZYL/631qBTScS+sQb9L7GKHczu1c9+ObEsmE5xyQBF9g1i0SiB/v1IxligpGMSFu1squfjsOZrBdGGCBaZ9rJQt7q1Uvi95QXA7ka6sAbxPvAppWypkutdytX9PY009C25SXzhG8Wo9MbcZF8g0dYJEyPnC+7OMf0Uz5EsqFGxSWhIqUSAc92mMye2YkI0Y1y+HUJUNAYjOK7rnaV34yL+bqOc3HQ/lFoCEpJ7AQmj+373z/CLRGJo+6ckBkptlFU2jCCbjeXE998MtBQ/OhjdibJBiQWuwiCf8ZQ1ggAVz4vl5gvMYRA9fC9MFW1ke2BbpnkAJZeUBDwM//TsK8R MonkeySphere2016-12-06T16:05:42 Michal Rydlikowski (Personal Secondary Email Address) <Michal.Rydlikowski@gmail.com>
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCqtzL0th+fvk8YFDr+r4evH8s/xEAtXqEEPJN0ldyESyO/H64az4QRYaZTgwbLR6nY9Zk0VBH1UQ2M35flYH1ZMQViDaZ/CKXoctLmCgWSc05IYQUf/afruBKMmCvEJ1YAqWyY1h9rtNX2amICygxzEdFG/Pg1tiM4reTgTFxQOu15knCWKoYzZQ6WhUcvFU+E7d/OFYEzIAil7oAZiDzVPRnNY/HLPqnlisBy1HNiBqaX/nM3tDyxYPLDvROS0O6jhUzXJ1VK72cJskwHusLWsbLRTtKdRX9WX+4yYCzJEoRKYTKRLBYInkHqLjOYs9vM/SovgGI9wtvRgeDTfFyB michal.rydlikowski@concreteplatform.com
write_files:
  - path: /etc/kubernetes/install.sh
    owner: "root"
    permissions: 0700
    content: |
      #!/bin/bash
      set -e
      
      # List of etcd servers (http://ip:port), comma separated
      export ETCD_ENDPOINTS=http://192.168.115.3:2379
      
      # Specify the version (vX.Y.Z) of Kubernetes assets to deploy
      export K8S_VER=v1.7.8_coreos.2
      
      # Hyperkube image repository to use.
      export HYPERKUBE_IMAGE_REPO=quay.io/coreos/hyperkube
      
      # The CIDR network to use for pod IPs.
      # Each pod launched in the cluster will be assigned an IP out of this range.
      # Each node will be configured such that these IPs will be routable using the flannel overlay network.
      export POD_NETWORK=10.2.0.0/16
      
      # The CIDR network to use for service cluster IPs.
      # Each service will be assigned a cluster IP out of this range.
      # This must not overlap with any IP ranges assigned to the POD_NETWORK, or other existing network infrastructure.
      # Routing to these IPs is handled by a proxy service local to each node, and are not required to be routable between nodes.
      export SERVICE_IP_RANGE=10.3.0.0/24
      
      # The IP address of the Kubernetes API Service
      # If the SERVICE_IP_RANGE is changed above, this must be set to the first IP in that range.
      export K8S_SERVICE_IP=10.3.0.1
      
      # The IP address of the cluster DNS service.
      # This IP must be in the range of the SERVICE_IP_RANGE and cannot be the first IP in the range.
      # This same IP must be configured on all worker nodes to enable DNS service discovery.
      export DNS_SERVICE_IP=10.3.0.10
      
      # Whether to use Calico for Kubernetes network policy.
      export USE_CALICO=true
      
      # Determines the container runtime for kubernetes to use. Accepts 'docker' or 'rkt'.
      export CONTAINER_RUNTIME=docker
      
      # The above settings can optionally be overridden using an environment file:
      ENV_FILE=/run/coreos-kubernetes/options.env
      
      # To run a self hosted Calico install it needs to be able to write to the CNI dir
      if [ "${USE_CALICO}" = "true" ]; then
          export CALICO_OPTS="--volume cni-bin,kind=host,source=/opt/cni/bin \
                              --mount volume=cni-bin,target=/opt/cni/bin"
      else
          export CALICO_OPTS=""
      fi
      
      # -------------
      
      function init_config {
          local REQUIRED=('ADVERTISE_IP' 'POD_NETWORK' 'ETCD_ENDPOINTS' 'SERVICE_IP_RANGE' 'K8S_SERVICE_IP' 'DNS_SERVICE_IP' 'K8S_VER' 'HYPERKUBE_IMAGE_REPO' 'USE_CALICO')
      
          if [ -f $ENV_FILE ]; then
              export $(cat $ENV_FILE | xargs)
          fi
      
          if [ -z $ADVERTISE_IP ]; then
              export ADVERTISE_IP=$(awk -F= '/COREOS_PUBLIC_IPV4/ {print $2}' /etc/environment)
          fi
      
          for REQ in "${REQUIRED[@]}"; do
              if [ -z "$(eval echo \$$REQ)" ]; then
                  echo "Missing required config value: ${REQ}"
                  exit 1
              fi
          done
      }
      
      function init_flannel {
          echo "Waiting for etcd..."
          while true
          do
              IFS=',' read -ra ES <<< "$ETCD_ENDPOINTS"
              for ETCD in "${ES[@]}"; do
                  echo "Trying: $ETCD"
                  if [ -n "$(curl --silent "$ETCD/v2/machines")" ]; then
                      local ACTIVE_ETCD=$ETCD
                      break
                  fi
                  sleep 1
              done
              if [ -n "$ACTIVE_ETCD" ]; then
                  break
              fi
          done
          RES=$(curl --silent -X PUT -d "value={\"Network\":\"$POD_NETWORK\",\"Backend\":{\"Type\":\"vxlan\"}}" "$ACTIVE_ETCD/v2/keys/coreos.com/network/config?prevExist=false")
          if [ -z "$(echo $RES | grep '"action":"create"')" ] && [ -z "$(echo $RES | grep 'Key already exists')" ]; then
              echo "Unexpected error configuring flannel pod network: $RES"
          fi
      }
      
      function init_templates {
      
        local TEMPLATE=/etc/kubernetes/master-kubeconfig.yaml
        if [ ! -f $TEMPLATE ]; then
            echo "TEMPLATE: $TEMPLATE"
            mkdir -p $(dirname $TEMPLATE)
            cat << EOF > $TEMPLATE
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          certificate-authority: /etc/kubernetes/ssl/ca.pem
          server: http://127.0.0.1:8080
      users:
      - name: kubelet
        user:
          client-certificate: /etc/kubernetes/ssl/apiserver.pem
          client-key: /etc/kubernetes/ssl/apiserver-key.pem
      contexts:
      - context:
          cluster: local
          user: kubelet
        name: kubelet-context
      current-context: kubelet-context
      EOF
        fi
      
          local TEMPLATE=/etc/systemd/system/kubelet.service
          local uuid_file="/var/run/kubelet-pod.uuid"
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      [Service]
      Environment=KUBELET_IMAGE_TAG=${K8S_VER}
      Environment=KUBELET_IMAGE_URL=${HYPERKUBE_IMAGE_REPO}
      Environment="RKT_RUN_ARGS=--uuid-file-save=${uuid_file} \
        --volume dns,kind=host,source=/etc/resolv.conf \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume rkt,kind=host,source=/opt/bin/host-rkt \
        --mount volume=rkt,target=/usr/bin/rkt \
        --volume var-lib-rkt,kind=host,source=/var/lib/rkt \
        --mount volume=var-lib-rkt,target=/var/lib/rkt \
        --volume stage,kind=host,source=/tmp \
        --mount volume=stage,target=/tmp \
        --volume var-log,kind=host,source=/var/log \
        --mount volume=var-log,target=/var/log \
        ${CALICO_OPTS}"
      ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
      ExecStartPre=/usr/bin/mkdir -p /opt/cni/bin
      ExecStartPre=/usr/bin/mkdir -p /var/log/containers
      ExecStartPre=-/usr/bin/rkt rm --uuid-file=${uuid_file}
      ExecStart=/usr/lib/coreos/kubelet-wrapper \
        --kubeconfig /etc/kubernetes/master-kubeconfig.yaml \
        --hostname-override=%HOST% \
        --require-kubeconfig=true \
        --register-schedulable=false \
        --container-runtime=${CONTAINER_RUNTIME} \
        --rkt-path=/usr/bin/rkt \
        --rkt-stage1-image=coreos.com/rkt/stage1-coreos \
        --allow-privileged=true \
        --pod-manifest-path=/etc/kubernetes/manifests \
        --hostname-override=${ADVERTISE_IP} \
        --cluster_dns=${DNS_SERVICE_IP} \
        --cluster_domain=cluster.local
      ExecStop=-/usr/bin/rkt stop --uuid-file=${uuid_file}
      Restart=always
      RestartSec=10
      
      [Install]
      WantedBy=multi-user.target
      EOF
          fi
      
          local TEMPLATE=/opt/bin/host-rkt
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      #!/bin/sh
      # This is bind mounted into the kubelet rootfs and all rkt shell-outs go
      # through this rkt wrapper. It essentially enters the host mount namespace
      # (which it is already in) only for the purpose of breaking out of the chroot
      # before calling rkt. It makes things like rkt gc work and avoids bind mounting
      # in certain rkt filesystem dependancies into the kubelet rootfs. This can
      # eventually be obviated when the write-api stuff gets upstream and rkt gc is
      # through the api-server. Related issue:
      # https://github.com/coreos/rkt/issues/2878
      exec nsenter -m -u -i -n -p -t 1 -- /usr/bin/rkt "\$@"
      EOF
          fi
      
      
          local TEMPLATE=/etc/systemd/system/load-rkt-stage1.service
          if [ ${CONTAINER_RUNTIME} = "rkt" ] && [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      [Unit]
      Description=Load rkt stage1 images
      Documentation=http://github.com/coreos/rkt
      Requires=network-online.target
      After=network-online.target
      Before=rkt-api.service
      
      [Service]
      RemainAfterExit=yes
      Type=oneshot
      ExecStart=/usr/bin/rkt fetch /usr/lib/rkt/stage1-images/stage1-coreos.aci /usr/lib/rkt/stage1-images/stage1-fly.aci  --insecure-options=image
      
      [Install]
      RequiredBy=rkt-api.service
      EOF
          fi
      
          local TEMPLATE=/etc/systemd/system/rkt-api.service
          if [ ${CONTAINER_RUNTIME} = "rkt" ] && [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      [Unit]
      Before=kubelet.service
      
      [Service]
      ExecStart=/usr/bin/rkt api-service
      Restart=always
      RestartSec=10
      
      [Install]
      RequiredBy=kubelet.service
      EOF
          fi
      
          local TEMPLATE=/etc/kubernetes/manifests/kube-proxy.yaml
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-proxy
        namespace: kube-system
        annotations:
          rkt.alpha.kubernetes.io/stage1-name-override: coreos.com/rkt/stage1-fly
      spec:
        hostNetwork: true
        containers:
        - name: kube-proxy
          image: ${HYPERKUBE_IMAGE_REPO}:$K8S_VER
          command:
          - /hyperkube
          - proxy
          - --master=http://127.0.0.1:8080
          - --cluster-cidr=${POD_NETWORK}
          securityContext:
            privileged: true
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
          - mountPath: /var/run/dbus
            name: dbus
            readOnly: false
        volumes:
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
        - hostPath:
            path: /var/run/dbus
          name: dbus
      EOF
          fi
      
          local TEMPLATE=/etc/kubernetes/manifests/kube-apiserver.yaml
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-apiserver
          image: ${HYPERKUBE_IMAGE_REPO}:$K8S_VER
          command:
          - /hyperkube
          - apiserver
          - --bind-address=0.0.0.0
          - --etcd-servers=${ETCD_ENDPOINTS}
          - --allow-privileged=true
          - --service-cluster-ip-range=${SERVICE_IP_RANGE}
          - --secure-port=443
          - --insecure-port=8080
          - --storage-backend=etcd2
          - --advertise-address=${ADVERTISE_IP}
          - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota
          - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
          - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --client-ca-file=/etc/kubernetes/ssl/ca.pem
          - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --runtime-config=extensions/v1beta1/networkpolicies=true
          - --anonymous-auth=false
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              port: 8080
              path: /healthz
            initialDelaySeconds: 15
            timeoutSeconds: 15
          ports:
          - containerPort: 443
            hostPort: 443
            name: https
          - containerPort: 8080
            hostPort: 8080
            name: local
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
      EOF
          fi
      
          local TEMPLATE=/etc/kubernetes/manifests/kube-controller-manager.yaml
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
      spec:
        containers:
        - name: kube-controller-manager
          image: ${HYPERKUBE_IMAGE_REPO}:$K8S_VER
          command:
          - /hyperkube
          - controller-manager
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --root-ca-file=/etc/kubernetes/ssl/ca.pem
          resources:
            requests:
              cpu: 200m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10252
            initialDelaySeconds: 15
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        hostNetwork: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
      EOF
          fi
      
          local TEMPLATE=/etc/kubernetes/manifests/kube-scheduler.yaml
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-scheduler
          image: ${HYPERKUBE_IMAGE_REPO}:$K8S_VER
          command:
          - /hyperkube
          - scheduler
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          resources:
            requests:
              cpu: 100m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
            timeoutSeconds: 15
      EOF
          fi
      
          local TEMPLATE=/srv/kubernetes/manifests/kube-dns-de.yaml
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      apiVersion: extensions/v1beta1
      kind: Deployment
      metadata:
        name: kube-dns
        namespace: kube-system
        labels:
          k8s-app: kube-dns
          kubernetes.io/cluster-service: "true"
      spec:
        strategy:
          rollingUpdate:
            maxSurge: 10%
            maxUnavailable: 0
        selector:
          matchLabels:
            k8s-app: kube-dns
        template:
          metadata:
            labels:
              k8s-app: kube-dns
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
          spec:
            containers:
            - name: kubedns
              image: gcr.io/google_containers/kubedns-amd64:1.9
              resources:
                limits:
                  memory: 170Mi
                requests:
                  cpu: 100m
                  memory: 70Mi
              livenessProbe:
                httpGet:
                  path: /healthz-kubedns
                  port: 8080
                  scheme: HTTP
                initialDelaySeconds: 60
                timeoutSeconds: 5
                successThreshold: 1
                failureThreshold: 5
              readinessProbe:
                httpGet:
                  path: /readiness
                  port: 8081
                  scheme: HTTP
                initialDelaySeconds: 3
                timeoutSeconds: 5
              args:
              - --domain=cluster.local.
              - --dns-port=10053
              - --config-map=kube-dns
              # This should be set to v=2 only after the new image (cut from 1.5) has
              # been released, otherwise we will flood the logs.
              - --v=2
              env:
              - name: PROMETHEUS_PORT
                value: "10055"
              ports:
              - containerPort: 10053
                name: dns-local
                protocol: UDP
              - containerPort: 10053
                name: dns-tcp-local
                protocol: TCP
              - containerPort: 10055
                name: metrics
                protocol: TCP
            - name: dnsmasq
              image: gcr.io/google_containers/kube-dnsmasq-amd64:1.4
              livenessProbe:
                httpGet:
                  path: /healthz-dnsmasq
                  port: 8080
                  scheme: HTTP
                initialDelaySeconds: 60
                timeoutSeconds: 5
                successThreshold: 1
                failureThreshold: 5
              args:
              - --cache-size=1000
              - --no-resolv
              - --server=127.0.0.1#10053
              - --log-facility=-
              ports:
              - containerPort: 53
                name: dns
                protocol: UDP
              - containerPort: 53
                name: dns-tcp
                protocol: TCP
              # see: https://github.com/kubernetes/kubernetes/issues/29055 for details
              resources:
                requests:
                  cpu: 150m
                  memory: 10Mi
            - name: dnsmasq-metrics
              image: gcr.io/google_containers/dnsmasq-metrics-amd64:1.0
              livenessProbe:
                httpGet:
                  path: /metrics
                  port: 10054
                  scheme: HTTP
                initialDelaySeconds: 60
                timeoutSeconds: 5
                successThreshold: 1
                failureThreshold: 5
              args:
              - --v=2
              - --logtostderr
              ports:
              - containerPort: 10054
                name: metrics
                protocol: TCP
              resources:
                requests:
                  memory: 10Mi
            - name: healthz
              image: gcr.io/google_containers/exechealthz-amd64:1.2
              resources:
                limits:
                  memory: 50Mi
                requests:
                  cpu: 10m
                  memory: 50Mi
              args:
              - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 >/dev/null
              - --url=/healthz-dnsmasq
              - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null
              - --url=/healthz-kubedns
              - --port=8080
              - --quiet
              ports:
              - containerPort: 8080
                protocol: TCP
            dnsPolicy: Default
      
      EOF
          fi
      
          local TEMPLATE=/srv/kubernetes/manifests/kube-dns-autoscaler-de.yaml
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      apiVersion: extensions/v1beta1
      kind: Deployment
      metadata:
        name: kube-dns-autoscaler
        namespace: kube-system
        labels:
          k8s-app: kube-dns-autoscaler
          kubernetes.io/cluster-service: "true"
      spec:
        template:
          metadata:
            labels:
              k8s-app: kube-dns-autoscaler
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
          spec:
            containers:
            - name: autoscaler
              image: gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.0.0
              resources:
                  requests:
                      cpu: "20m"
                      memory: "10Mi"
              command:
                - /cluster-proportional-autoscaler
                - --namespace=kube-system
                - --configmap=kube-dns-autoscaler
                - --mode=linear
                - --target=Deployment/kube-dns
                - --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"min":1}}
                - --logtostderr=true
                - --v=2
      EOF
          fi
      
          local TEMPLATE=/srv/kubernetes/manifests/kube-dns-svc.yaml
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      apiVersion: v1
      kind: Service
      metadata:
        name: kube-dns
        namespace: kube-system
        labels:
          k8s-app: kube-dns
          kubernetes.io/cluster-service: "true"
          kubernetes.io/name: "KubeDNS"
      spec:
        selector:
          k8s-app: kube-dns
        clusterIP: ${DNS_SERVICE_IP}
        ports:
        - name: dns
          port: 53
          protocol: UDP
        - name: dns-tcp
          port: 53
          protocol: TCP
      EOF
          fi
      
          local TEMPLATE=/srv/kubernetes/manifests/heapster-de.yaml
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      apiVersion: extensions/v1beta1
      kind: Deployment
      metadata:
        name: heapster-v1.2.0
        namespace: kube-system
        labels:
          k8s-app: heapster
          kubernetes.io/cluster-service: "true"
          version: v1.2.0
      spec:
        replicas: 1
        selector:
          matchLabels:
            k8s-app: heapster
            version: v1.2.0
        template:
          metadata:
            labels:
              k8s-app: heapster
              version: v1.2.0
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
          spec:
            containers:
              - image: gcr.io/google_containers/heapster:v1.2.0
                name: heapster
                livenessProbe:
                  httpGet:
                    path: /healthz
                    port: 8082
                    scheme: HTTP
                  initialDelaySeconds: 180
                  timeoutSeconds: 5
                command:
                  - /heapster
                  - --source=kubernetes.summary_api:''
              - image: gcr.io/google_containers/addon-resizer:1.6
                name: heapster-nanny
                resources:
                  limits:
                    cpu: 50m
                    memory: 90Mi
                  requests:
                    cpu: 50m
                    memory: 90Mi
                env:
                  - name: MY_POD_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.name
                  - name: MY_POD_NAMESPACE
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.namespace
                command:
                  - /pod_nanny
                  - --cpu=80m
                  - --extra-cpu=4m
                  - --memory=200Mi
                  - --extra-memory=4Mi
                  - --threshold=5
                  - --deployment=heapster-v1.2.0
                  - --container=heapster
                  - --poll-period=300000
                  - --estimator=exponential
      EOF
          fi
      
          local TEMPLATE=/srv/kubernetes/manifests/heapster-svc.yaml
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      kind: Service
      apiVersion: v1
      metadata:
        name: heapster
        namespace: kube-system
        labels:
          kubernetes.io/cluster-service: "true"
          kubernetes.io/name: "Heapster"
      spec:
        ports:
          - port: 80
            targetPort: 8082
        selector:
          k8s-app: heapster
      EOF
          fi
      
          local TEMPLATE=/srv/kubernetes/manifests/kube-dashboard-de.yaml
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      apiVersion: extensions/v1beta1
      kind: Deployment
      metadata:
        name: kubernetes-dashboard
        namespace: kube-system
        labels:
          k8s-app: kubernetes-dashboard
          kubernetes.io/cluster-service: "true"
      spec:
        selector:
          matchLabels:
            k8s-app: kubernetes-dashboard
        template:
          metadata:
            labels:
              k8s-app: kubernetes-dashboard
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
          spec:
            containers:
            - name: kubernetes-dashboard
              image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.5.0
              resources:
                # keep request = limit to keep this container in guaranteed class
                limits:
                  cpu: 100m
                  memory: 50Mi
                requests:
                  cpu: 100m
                  memory: 50Mi
              ports:
              - containerPort: 9090
              livenessProbe:
                httpGet:
                  path: /
                  port: 9090
                initialDelaySeconds: 30
                timeoutSeconds: 30
      EOF
          fi
      
          local TEMPLATE=/srv/kubernetes/manifests/kube-dashboard-svc.yaml
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      apiVersion: v1
      kind: Service
      metadata:
        name: kubernetes-dashboard
        namespace: kube-system
        labels:
          k8s-app: kubernetes-dashboard
          kubernetes.io/cluster-service: "true"
      spec:
        selector:
          k8s-app: kubernetes-dashboard
        ports:
        - port: 80
          targetPort: 9090
      EOF
          fi
      
          local TEMPLATE=/etc/flannel/options.env
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      FLANNELD_IFACE=$ADVERTISE_IP
      FLANNELD_ETCD_ENDPOINTS=$ETCD_ENDPOINTS
      EOF
          fi
      
          local TEMPLATE=/etc/systemd/system/flanneld.service.d/40-ExecStartPre-symlink.conf.conf
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      [Service]
      ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
      EOF
          fi
      
          local TEMPLATE=/etc/systemd/system/docker.service.d/40-flannel.conf
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      [Unit]
      Requires=flanneld.service
      After=flanneld.service
      [Service]
      EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
      EOF
          fi
      
          local TEMPLATE=/etc/kubernetes/cni/docker_opts_cni.env
          if [ ! -f $TEMPLATE ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      DOCKER_OPT_BIP=""
      DOCKER_OPT_IPMASQ=""
      EOF
          fi
      
          local TEMPLATE=/etc/kubernetes/cni/net.d/10-flannel.conf
          if [ "${USE_CALICO}" = "false" ] && [ ! -f "${TEMPLATE}" ]; then
              echo "TEMPLATE: $TEMPLATE"
              mkdir -p $(dirname $TEMPLATE)
              cat << EOF > $TEMPLATE
      {
          "name": "podnet",
          "type": "flannel",
          "delegate": {
              "isDefaultGateway": true
          }
      }
      EOF
          fi
      
          local TEMPLATE=/srv/kubernetes/manifests/calico.yaml
          if [ "${USE_CALICO}" = "true" ]; then
          echo "TEMPLATE: $TEMPLATE"
          mkdir -p $(dirname $TEMPLATE)
          cat << EOF > $TEMPLATE
      # This ConfigMap is used to configure a self-hosted Calico installation.
      kind: ConfigMap
      apiVersion: v1
      metadata:
        name: calico-config
        namespace: kube-system
      data:
        # Configure this with the location of your etcd cluster.
        etcd_endpoints: "${ETCD_ENDPOINTS}"
      
        # The CNI network configuration to install on each node.  The special
        # values in this config will be automatically populated.
        cni_network_config: |-
          {
              "name": "calico",
              "type": "flannel",
              "delegate": {
                "type": "calico",
                "etcd_endpoints": "__ETCD_ENDPOINTS__",
                "log_level": "info",
                "policy": {
                    "type": "k8s",
                    "k8s_api_root": "https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__",
                    "k8s_auth_token": "__SERVICEACCOUNT_TOKEN__"
                },
                "kubernetes": {
                    "kubeconfig": "/etc/kubernetes/cni/net.d/__KUBECONFIG_FILENAME__"
                }
              }
          }
      
      ---
      
      # This manifest installs the calico/node container, as well
      # as the Calico CNI plugins and network config on
      # each master and worker node in a Kubernetes cluster.
      kind: DaemonSet
      apiVersion: extensions/v1beta1
      metadata:
        name: calico-node
        namespace: kube-system
        labels:
          k8s-app: calico-node
      spec:
        selector:
          matchLabels:
            k8s-app: calico-node
        template:
          metadata:
            labels:
              k8s-app: calico-node
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              scheduler.alpha.kubernetes.io/tolerations: |
                [{"key": "dedicated", "value": "master", "effect": "NoSchedule" },
                 {"key":"CriticalAddonsOnly", "operator":"Exists"}]
          spec:
            hostNetwork: true
            containers:
              # Runs calico/node container on each Kubernetes node.  This
              # container programs network policy and routes on each
              # host.
              - name: calico-node
                image: quay.io/calico/node:v2.6.2
                env:
                  # The location of the Calico etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  # Choose the backend to use.
                  - name: CALICO_NETWORKING_BACKEND
                    value: "none"
                  # Disable file logging so 'kubectl logs' works.
                  - name: CALICO_DISABLE_FILE_LOGGING
                    value: "true"
                  - name: NO_DEFAULT_POOLS
                    value: "true"
                securityContext:
                  privileged: true
                volumeMounts:
                  - mountPath: /lib/modules
                    name: lib-modules
                    readOnly: false
                  - mountPath: /var/run/calico
                    name: var-run-calico
                    readOnly: false
                  - mountPath: /etc/resolv.conf
                    name: dns
                    readOnly: true
              # This container installs the Calico CNI binaries
              # and CNI network config file on each node.
              - name: install-cni
                image: quay.io/calico/cni:latest
                imagePullPolicy: Always
                command: ["/install-cni.sh"]
                env:
                  # CNI configuration filename
                  - name: CNI_CONF_NAME
                    value: "10-calico.conf"
                  # The location of the Calico etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  # The CNI network config to install on each node.
                  - name: CNI_NETWORK_CONFIG
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: cni_network_config
                volumeMounts:
                  - mountPath: /host/opt/cni/bin
                    name: cni-bin-dir
                  - mountPath: /host/etc/cni/net.d
                    name: cni-net-dir
            volumes:
              # Used by calico/node.
              - name: lib-modules
                hostPath:
                  path: /lib/modules
              - name: var-run-calico
                hostPath:
                  path: /var/run/calico
              # Used to install CNI.
              - name: cni-bin-dir
                hostPath:
                  path: /opt/cni/bin
              - name: cni-net-dir
                hostPath:
                  path: /etc/kubernetes/cni/net.d
              - name: dns
                hostPath:
                  path: /etc/resolv.conf
      
      ---
      
      # This manifest deploys the Calico policy controller on Kubernetes.
      # See https://github.com/projectcalico/k8s-policy
      apiVersion: extensions/v1beta1
      kind: ReplicaSet
      metadata:
        name: calico-policy-controller
        namespace: kube-system
        labels:
          k8s-app: calico-policy
      spec:
        # The policy controller can only have a single active instance.
        replicas: 1
        template:
          metadata:
            name: calico-policy-controller
            namespace: kube-system
            labels:
              k8s-app: calico-policy
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              scheduler.alpha.kubernetes.io/tolerations: |
                [{"key": "dedicated", "value": "master", "effect": "NoSchedule" },
                 {"key":"CriticalAddonsOnly", "operator":"Exists"}]
          spec:
            # The policy controller must run in the host network namespace so that
            # it isn't governed by policy that would prevent it from working.
            hostNetwork: true
            containers:
              - name: calico-policy-controller
                image: calico/kube-policy-controller:v0.4.0
                env:
                  # The location of the Calico etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  # The location of the Kubernetes API.  Use the default Kubernetes
                  # service for API access.
                  - name: K8S_API
                    value: "https://kubernetes.default:443"
                  # Since we're running in the host namespace and might not have KubeDNS
                  # access, configure the container's /etc/hosts to resolve
                  # kubernetes.default to the correct service clusterIP.
                  - name: CONFIGURE_ETC_HOSTS
                    value: "true"
      EOF
          fi
      }
      
      function start_addons {
          echo "Waiting for Kubernetes API..."
          until curl --silent "http://127.0.0.1:8080/version"
          do
              sleep 5
          done
      
          echo
          echo "K8S: DNS addon"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/kube-dns-de.yaml)" "http://127.0.0.1:8080/apis/extensions/v1beta1/namespaces/kube-system/deployments" > /dev/null
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/kube-dns-svc.yaml)" "http://127.0.0.1:8080/api/v1/namespaces/kube-system/services" > /dev/null
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/kube-dns-autoscaler-de.yaml)" "http://127.0.0.1:8080/apis/extensions/v1beta1/namespaces/kube-system/deployments" > /dev/null
          echo "K8S: Heapster addon"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/heapster-de.yaml)" "http://127.0.0.1:8080/apis/extensions/v1beta1/namespaces/kube-system/deployments" > /dev/null
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/heapster-svc.yaml)" "http://127.0.0.1:8080/api/v1/namespaces/kube-system/services" > /dev/null
          echo "K8S: Dashboard addon"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/kube-dashboard-de.yaml)" "http://127.0.0.1:8080/apis/extensions/v1beta1/namespaces/kube-system/deployments" > /dev/null
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/kube-dashboard-svc.yaml)" "http://127.0.0.1:8080/api/v1/namespaces/kube-system/services" > /dev/null
      }
      
      function start_calico {
          echo "Waiting for Kubernetes API..."
          # wait for the API
          until curl --silent "http://127.0.0.1:8080/version/"
          do
              sleep 5
          done
          echo "Deploying Calico"
          # Deploy Calico
          #TODO: change to rkt once this is resolved (https://github.com/coreos/rkt/issues/3181)
          docker run --rm --net=host -v /srv/kubernetes/manifests:/host/manifests $HYPERKUBE_IMAGE_REPO:$K8S_VER /hyperkube kubectl apply -f /host/manifests/calico.yaml
      }
      
      init_config
      init_templates
      
      chmod +x /opt/bin/host-rkt
      
      init_flannel
      
      systemctl stop update-engine; systemctl mask update-engine
      
      systemctl daemon-reload
      
      if [ $CONTAINER_RUNTIME = "rkt" ]; then
              systemctl enable load-rkt-stage1
              systemctl enable rkt-api
      fi
      
      systemctl enable flanneld; systemctl start flanneld
      
      systemctl enable kubelet; systemctl start kubelet
      
      if [ $USE_CALICO = "true" ]; then
              start_calico
      fi
      
      start_addons
      echo "DONE"

  - path: /etc/kubernetes/ssl/ca.pem
    owner: "root"
    permissions: 0600
    content: |
      -----BEGIN CERTIFICATE-----
      MIIC+jCCAeKgAwIBAgIJAPh17scJCGbBMA0GCSqGSIb3DQEBCwUAMBIxEDAOBgNV
      BAMMB2t1YmUtY2EwHhcNMTcxMDI4MTM1NzM3WhcNNDUwMzE1MTM1NzM3WjASMRAw
      DgYDVQQDDAdrdWJlLWNhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA
      xRbtt+vFNE/FsiQh1T/Tj9nM1Oj+De1AeilFeFLWqu0OTQAHS2d9hzJJ11Nbb+ve
      Susj52LRJKK6Z7sFvXtGCiQ8rVJrKSgvQrejsXAI37L/QHPDlxVj3tEXFcQITlai
      RGryZBNkQnyQtBUVY7klPfLMEMlWITLUUd40KGGThSO40O7pb1k2qa0cGxhfC6Om
      8ykxs+mZrYKrb+BhH/8iGVubKvbMAiqSk+GWtsEq3Z0VeJo1lk7iW9PtV5VtZez8
      6a+1cv0Z+DUysfDkPTAFCq0bGWz9ezMLvqB+3O5I24jfc+1HnSxXehvC0FvEuTpe
      GBOOrj6h1ZLPWbbUGlcmBwIDAQABo1MwUTAdBgNVHQ4EFgQUyBK9vD42B5puimsJ
      0pfqVnPDRk0wHwYDVR0jBBgwFoAUyBK9vD42B5puimsJ0pfqVnPDRk0wDwYDVR0T
      AQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAOF9AT0Ejzlowl154tCqBgBzZ
      zqwihPmiKlScIEPpwmcp+xG/xfAhnekMxK5uONLZrdEjbofKkXGIGtY0DpEwdNK8
      WXMQQlz37pOB/NwDmRW/HN4kKzLqnRWwUHiwL8FrtDJ1Z9WerzRArb9ARd7IMYwO
      xo3p6NKvO849aFdmlY+Agm2AZxHJGjTQVJR2CdtF9XDu9z1EkwB3mKILf9eqeN0w
      2JywAlOZQhz0qiSQPDEgBv7GeCvqnyJ3gdE41ZGgAiAlODSMtqYVeaSjHIZw+LNA
      Mm/vt2HvYiMnGNjmTfOmentc3tLAeTIEzxvINmzVhSuv3AAsrUxdgttrWBPfhQ==
      -----END CERTIFICATE-----

  - path: /etc/kubernetes/ssl/apiserver.pem
    owner: "root"
    permissions: 0600
    content: |
      -----BEGIN CERTIFICATE-----
      MIIDPzCCAiegAwIBAgIJAL2GgDxUDuYxMA0GCSqGSIb3DQEBCwUAMBIxEDAOBgNV
      BAMMB2t1YmUtY2EwHhcNMTcxMDI4MTM1NzM4WhcNMjcxMDI2MTM1NzM4WjAZMRcw
      FQYDVQQDDA5rdWJlLWFwaXNlcnZlcjCCASIwDQYJKoZIhvcNAQEBBQADggEPADCC
      AQoCggEBALMILully6C+0Vn1zQZXuNt69X2fusOREEX5UBvx5fKevHRP7S2kC9br
      4c90mUXrWSIeSzjh2iTRF5MdJdgPufpy4gqb+u+tMD5BXpsu1nULSV4PTmdKNX1r
      PO7cC4rBeMVoult9aAIbKXLwAjNydpec0FyHfZVvW8Mi543IBgzjUabghTMrleIe
      PlJqecYMJjx6TBn/zYzom4O6LfV/qlBqoVJSmxf0qcSXlMf2KMvBl1zFy4yU1OCx
      w1RHgDJN8MtaWMBMfL71FGX2CZEplpnJhNsnwv2j8mJSy/GeVFOpl62bjjxio3BM
      emzkzouw7kjJP4xechPj6fJOi8TOoEECAwEAAaOBkDCBjTAJBgNVHRMEAjAAMAsG
      A1UdDwQEAwIF4DBzBgNVHREEbDBqggprdWJlcm5ldGVzghJrdWJlcm5ldGVzLmRl
      ZmF1bHSCFmt1YmVybmV0ZXMuZGVmYXVsdC5zdmOCJGt1YmVybmV0ZXMuZGVmYXVs
      dC5zdmMuY2x1c3Rlci5sb2NhbIcECgMAAYcEwKhzAzANBgkqhkiG9w0BAQsFAAOC
      AQEAJWF+z5vRFzlcG30hYczrbIw/vvnazW01dD2lAeffmNJyKGhHw8DcfV3npOg9
      IQdhcycLYoPTVmWD8hfTqBpmH2PDW50MZxivZCALe2yqlUVo7+Si4dWuC9nJuaOJ
      VNCnZJ9brHhndrlu1YY2U7awmd3H/NREpc+l1tIB4iidi86SReWLsUdBoj6pgY2A
      UHRCUq1vrayciMns7p8fiCq1PCueVwtxw0ZY5Cglct59ZkxgUDzzLpwN4ETR8KxE
      jhpwWseiVj3wQqBnUMCu2VN6PTA7oxCn1AWFLBn/7tFQjfEKuzSD3DGrDUCoENkh
      S0B+S9fZlneu3xlWOukdihr+Uw==
      -----END CERTIFICATE-----

  - path: /etc/kubernetes/ssl/apiserver-key.pem
    owner: "root"
    permissions: 0600
    content: |
      -----BEGIN RSA PRIVATE KEY-----
      MIIEowIBAAKCAQEAswgu6WXLoL7RWfXNBle423r1fZ+6w5EQRflQG/Hl8p68dE/t
      LaQL1uvhz3SZRetZIh5LOOHaJNEXkx0l2A+5+nLiCpv6760wPkFemy7WdQtJXg9O
      Z0o1fWs87twLisF4xWi6W31oAhspcvACM3J2l5zQXId9lW9bwyLnjcgGDONRpuCF
      MyuV4h4+Ump5xgwmPHpMGf/NjOibg7ot9X+qUGqhUlKbF/SpxJeUx/Yoy8GXXMXL
      jJTU4LHDVEeAMk3wy1pYwEx8vvUUZfYJkSmWmcmE2yfC/aPyYlLL8Z5UU6mXrZuO
      PGKjcEx6bOTOi7DuSMk/jF5yE+Pp8k6LxM6gQQIDAQABAoIBABYI0c1EDBXSpm2O
      BRfcwBGzRneZqyDav8f88VEfKo/73XHrTGOrpxFior4nuZ2TY7sBQ3zSv0h8u4wN
      sFSxStB6RzvLCs1o4WEoJn6u+xIYu4TqGVo5FfBDvJIhXwJK4ZRaWWVdP+nScf2F
      cJ/oXJQ4ZqScwGYd+ItBxoH6tlPribLcEQVLRVtRMDkl9XaIFvanNPF+vrwwuxlS
      SgG6sMh4GiIGm+3wFb0FDzyRLbRJzPH5LiXDta2SKQe94pHF2cZg+/BA8ImZ3w44
      awWws3RLtOferhyCQ7HbzQOo82UGQVbyTKy6SbMGXDkGkWbA3rA6rS2TEjteegMF
      86N+L4kCgYEA2WlR79cqAnsgGhGCUvpB1fg3PqRqiDs9z1ZddzmABxhA0Ik7CoTI
      k78p7OeJ1apA8/ytp40BiAJuWH1CHbYP2Ai+bM0e0z3FNlM2yXPolHM69g4erOa0
      v435jpMbbjCQovAeDd2qXs6KKziehY8wS1Uyh30SJ39SMtqdCWy+zUcCgYEA0s79
      GXlSrYkwbfBZgDf315MYF4eeyPBwO0JUKnN8iu9Dsd5VQ+DNpL6QPgKPayRPqKat
      DV7r3g+3bteQMpJRqWpaVy+EN/3nrI8M5/E1zNzCsnzxB4QJyeYzkhBPQl51fjEX
      kgwbRxDXN4wJhlN02P+bHh8owDAgjnvL9TC2SjcCgYByCdR4wZu/gWXtRpHdhHdU
      DMatjQB0RN6/CsYr63VEQ2x79f+KHOw402TiZvDdqlg3oqpT4JqVBPFsaQdtHjph
      TvTTjSRGGXiAblWmha3RcD/VIMLmXn0gnQ/xwSf6PG8hiGG7VDIxWJUQ5rn/xIPI
      f//pIamMHjS3iAQnFWtfsQKBgHF8bvLUxnaXoVO+DWCiQLF0BYc9QnToQ3MTbR4B
      V+2viuDpUTuefPaQhDRtymmifaPMPuMw04eTLGvmbkFbVcz/bSz8bpB6Bst1ozwG
      NbFYsxPFgThLk712SRL75r9S/Kt3oDnGfLTQUxwbLlF7Ai+Dwyz/F0AHaK9r48vW
      cnwXAoGBAMJJDRV1uh0ky8+iYENc6/0V8ySzFj1rgoO950ijBAY686ZCf7eMdEbI
      l+tYMqHL9TQHJU23HeYkLJaQDNnexyOefl0yqTPrkvqYBSIL3E7wso45P58RLbYG
      q5nGZCvLyUS6NQBlGFjaqfXQuYB8lJNBF7TLWIPGSiiueO6vd845
      -----END RSA PRIVATE KEY-----

  - path: /etc/kubernetes/cni/docker_opts_cni.env
coreos:
  units:
    - name: update-engine.service
      mask: true
      command: stop
    - name: locksmithd.service
      mask: true
      command: stop
    - name: 00-controller2.network
      content: |
        [Match]
        Name=eth0

        [Network]
        Address=192.168.115.3/24
        Gateway=192.168.115.1
        DNS=8.8.8.8
    - name: fleet.service
      command: start
    - name: etcd2.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=etcd2
        Conflicts=etcd.service
        [Service]
        User=etcd
        Type=notify
        Environment="ETCD_ADVERTISE_CLIENT_URLS=http://192.168.115.3:2379"
        Environment="ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379"
        Environment="ETCD_LISTEN_PEER_URLS=http://192.168.115.3:2380"
        Environment=ETCD_DATA_DIR=/var/lib/etcd2
        Environment=ETCD_NAME=controller2
        Environment=ETCD_INITIAL_CLUSTER_STATE=new
        Environment=ETCD_INITIAL_CLUSTER="controller1=http://192.168.115.2:2380,controller2=http://192.168.115.3:2380,controller3=http://192.168.115.4:2380"
        Environment="ETCD_INITIAL_ADVERTISE_PEER_URLS=http://192.168.115.3:2380"
        ExecStart=/usr/bin/etcd2
        Restart=always
        RestartSec=10s
        LimitNOFILE=40000
        TimeoutStartSec=0
        [Install]
        WantedBy=multi-user.target
    - name: haproxy.service
      enable: false
      content: |
        [Unit]
        Description=HAPROXY Kubernetes API
        After=docker.service
        Requires=docker.service

        [Service]
        ExecStart=/usr/bin/docker run --rm -e "MASTERS=192.168.115.2:443 192.168.115.3:443" -p 127.0.0.1:8182:8182 --name kubeapihaproxy --cap-add=SYS_ADMIN --cap-add DAC_READ_SEARCH --tmpfs /run --tmpfs /run/lock -v /sys/fs/cgroup:/sys/fs/cgroup:ro -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/:ro concreteplatform/kubeapihaproxy
        ExecStop=/usr/bin/docker stop kubeapihaproxy

        [Install]
        WantedBy=multi-user.target
    - name: fleet.service
      command: start
      enable: true
    - name: kubeinstall.service
      command: start
      content: |
        [Unit]
        Description=K8S installer
        After=etcd2.service
        Requires=etcd2.service

        [Service]
        Type=oneshot
        Environment=ADVERTISE_IP=192.168.115.3
        ExecStart=/etc/kubernetes/install.sh
